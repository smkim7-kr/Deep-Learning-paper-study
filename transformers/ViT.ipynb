{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ViT.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOLIvM4UudZ/dgPVmRsXvtn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"EX9iZ845Lp7_","executionInfo":{"status":"ok","timestamp":1641275558608,"user_tz":-540,"elapsed":7526,"user":{"displayName":"Sukmin Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12508020700810544909"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"code","source":["class PatchEmbed(nn.Module): # Normally use nn.Embedding instead\n","  \"\"\"\n","  Parameters\n","  img_size : Size of the image\n","  patch_size : Size of the patch\n","  in_chans: Number of input channels\n","  embed_dim: Embedding dimension\n","\n","  Attributes\n","  n_patchs: Number of patches inside of our image\n","  proj: Convolutional layer that splits into patches + embedding\n","  \"\"\"\n","  def __init__(self, img_size, patch_size, in_chans=3, embed_dim=768):\n","    super().__init__()\n","    self.img_size = img_size\n","    self.patch_size = patch_size\n","    assert img_size % patch_size == 0, \"Error on patch size\"\n","    self.n_patches = (img_size // patch_size) ** 2\n","\n","    self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) # Linear Projection of Flattened Patches\n","  \n","  def forward(self, x):\n","    \"\"\"Run forward pass.\n","    Parameters\n","    x: (n_samples, in_chans, img_size, img_size)\n","    \n","    Returns\n","    torch.Tensor(n_samples, n_patches, embed_dim)\n","    \"\"\"\n","    x = self.proj(x) # (n_samples, embed_dim, sqrt(n_patches), sqrt(n_patches))\n","    x = x.flatten(2) # (n_samples, embed_dim, n_patches)\n","    x = x.transpose(1, 2) # (n_samples, n_patches, embed_dim)\n","\n","    return x"],"metadata":{"id":"uITx4bA8Tf-m","executionInfo":{"status":"ok","timestamp":1641275558609,"user_tz":-540,"elapsed":10,"user":{"displayName":"Sukmin Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12508020700810544909"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class Attention(nn.Module):\n","  \"\"\"\n","  Parameters\n","  dim: Input and output dimension of per token features\n","  n_heads: Number of attention heads\n","  qkv_bias: Whether to include bias to qkv projections\n","  attn_p: Dropout probability applied to the qkv tensors\n","  proj_p: Dropout probability applied to the output tensor\n","\n","  Attributes\n","  scale: Normalizing constant for the drop product\n","  qkv: Linear projection for the qkv\n","  proj: Linear mapping that inputs concatenated output of all attention heads\n","  attn_drop, proj_drop: Dropout layers\n","  \"\"\"\n","  def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n","    super().__init__()\n","    self.n_heads = n_heads\n","    self.dim = dim # Feature of attention: same input and output dim\n","    self.head_dim = dim // n_heads\n","    self.scale = self.head_dim ** -0.5 # prevents extreme values using softmax - saturation problem\n","\n","    self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias)\n","    self.attn_drop = nn.Dropout(attn_p)\n","    self.proj = nn.Linear(dim, dim)\n","    self.proj_drop = nn.Dropout(proj_p)\n","\n","  def forward(self, x):\n","    \"\"\"Run forward pass\n","    Parameters\n","    x: (n_samples, n_patches+1, dim), +1 due to cls token\n","\n","    Returns\n","    torch.tensor(n_samples, n_patches+1, dim)\n","    \"\"\"\n","    n_samples, n_tokens, dim = x.shape\n","    assert dim == self.dim\n","\n","    qkv = self.qkv(x) # (n_samples, n_patches+1, 3*dim)\n","    qkv = qkv.reshape(n_samples, n_tokens, 3, self.n_heads, self.head_dim) # (n_samples, n_patches+1, 3, n_heads, head_dim)\n","    qkv = qkv.permute(2, 0, 3, 1, 4) # (3, n_samples, n_heads, n_patches+1, head_dim)\n","\n","    q, k, v  = qkv[0], qkv[1], qkv[2]\n","    k_t = k.transpose(-2, -1) # to compute dot product\n","    dp = (q @ k_t) * self.scale # (n_samples, n_heads, n_patches+1, n_patches+1)\n","    attn = dp.softmax(dim=-1) # (n_samples, n_heads, n_patches+1, n_patches+1)\n","    attn = self.attn_drop(attn)\n","\n","    weighted_avg = attn @ v # (n_samples, n_heads, n_patches+1, head_dim)\n","    weighted_avg = weighted_avg.transpose(1, 2) # (n_samples, n_patches+1, n_heads, head_dim)\n","    weighted_avg = weighted_avg.flatten(2) # (n_samples, n_patches+1, dim)\n","    x = self.proj(weighted_avg) # (n_samples, n_patches+1, dim)\n","    x = self.proj_drop(x)\n","\n","    return x"],"metadata":{"id":"cy8R7brUWl9J","executionInfo":{"status":"ok","timestamp":1641275558610,"user_tz":-540,"elapsed":9,"user":{"displayName":"Sukmin Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12508020700810544909"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class MLP(nn.Module):\n","  \"\"\"\n","  Parameters\n","  in_features: Number of input/output features\n","  hidden_features: Number of hidden features\n","  p: Dropout probability\n","\n","  Attribute\n","  fc: First linear layer\n","  act: GELU activation function\n","  fc2: Second linear layer\n","  drop: Dropout layer\n","  \"\"\"\n","  def __init__(self, features, hidden_features, p=0.):\n","    super().__init__()\n","    self.fc1 = nn.Linear(features, hidden_features)\n","    self.act = nn.GELU()\n","    self.fc2 = nn.Linear(hidden_features, features)\n","    self.drop = nn.Dropout(p)\n","\n","  def forward(self, x):\n","    \"\"\"Run forward pass.\n","    Parameters\n","    x: (n_samples, n_patches+1, features)\n","\n","    Returns\n","    torch.tensor(n_samples, n_patches+1, features)\n","    \"\"\"\n","    x = self.fc1(x) # (n_samples, n_patches+1, hidden_features)\n","    x = self.act(x)\n","    x = self.drop(x)\n","    x = self.fc2(x) # (n_samples, n_patches+1,features)\n","    x = self.drop(x)\n","\n","    return x"],"metadata":{"id":"jfRxn8ZVeQlz","executionInfo":{"status":"ok","timestamp":1641275558611,"user_tz":-540,"elapsed":9,"user":{"displayName":"Sukmin Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12508020700810544909"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class Block(nn.Module):\n","  \"\"\"\n","  Parameters\n","  dim: Embedding dimension\n","  n_heads: Number of attention heads\n","  mlp_ratio: hidden dimension size of MLP module respect to dim\n","  qkv_bias: Whether to include bias to qkv projections\n","  p, attn_p: Dropout probability\n","\n","  Attributes\n","  norm1, norm2; Layer normalization\n","  attn: Attention module\n","  mlp: MLP module\n","  \"\"\"\n","  def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.):\n","    super().__init__()\n","    self.norm1 = nn.LayerNorm(dim, eps=1e-6) # Always last dimension is normalized\n","    self.attn = Attention(dim, n_heads=n_heads, qkv_bias=qkv_bias, attn_p=attn_p, proj_p=p)\n","    self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n","    hidden_features = int(dim * mlp_ratio)\n","    self.mlp = MLP(features=dim, hidden_features=hidden_features)\n","  \n","  def forward(self, x):\n","    \"\"\"Run forward pass\n","    Parameters\n","    x: (n_samples, n_patches+1, dim)\n","\n","    Returns\n","    torch.tensor(n_samples, n_patches+1, dim)\n","    \"\"\" \n","    x = x + self.attn(self.norm1(x))\n","    x = x + self.mlp(self.norm2(x))\n","\n","    return x"],"metadata":{"id":"yl2wwIvLfR6T","executionInfo":{"status":"ok","timestamp":1641275559019,"user_tz":-540,"elapsed":416,"user":{"displayName":"Sukmin Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12508020700810544909"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class VisionTransformer(nn.Module):\n","  \"\"\"\n","  Parameters\n","  img_size: Height/Width of image\n","  patch_size: Height/Width of patch\n","  in_chans: Number of input channels\n","  n_classes: Number of classes\n","  embed_dim: Dimensionality of the token/patch embeddings\n","  depth: Number of blocks\n","  n_heads: Number of attention heads\n","  mlp_ratio: hidden dimension size of MLP module respect to dim\n","  qkv_bias: Whether to include bias to qkv projections\n","  p, attn_p: Dropout probability\n","\n","  Attributes\n","  patch_embed: Instance of PatchEmbed layer\n","  cls_token: Learnable parameter represent first token in the sequence\n","  pos_emb: Positional embedding of cls_token + all the pacehs (n_patches+1)*embed_dim\n","  pos_drop: Dropout layer\n","  blocks: List of Block modules\n","  norm: Layer normalization\n","  \"\"\"\n","  def __init__(self, img_size=384, patch_size=16, in_chans=3, n_classes=1000, embed_dim=768,\n","               depth=12, n_heads=12, mlp_ratio=4., qkv_bias=True, p=0., attn_p=0.):\n","    super().__init__()\n","    self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n","    self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # first two dims for convinience\n","    self.pos_embed = nn.Parameter(torch.zeros(1, 1+self.patch_embed.n_patches, embed_dim))\n","    self.pos_drop = nn.Dropout(p=p)\n","    \n","    self.blocks = nn.ModuleList(\n","        [\n","        Block(dim=embed_dim, n_heads=n_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, p=p, attn_p=attn_p)\n","        for _ in range(depth)\n","        ]\n","    )\n","\n","    self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n","    self.head = nn.Linear(embed_dim, n_classes)\n","\n","  def forward(self, x):\n","    \"\"\"Run forward pass.\n","    Parameters\n","    x: (n_samples, in_chans, img_size, img_size)\n","    \n","    Returns\n","    torch.tensor(n_samples, n_classes)\n","    \"\"\"\n","    n_samples = x.shape[0]\n","    x = self.patch_embed(x) \n","\n","    cls_token = self.cls_token.expand(n_samples, -1, -1) # (n_samples, 1, embed_dim)\n","    x = torch.cat((cls_token, x), dim=1) # (n_samples, 1+n_patches, embed_dim)\n","    x = x + self.pos_embed # (n_samples, 1+n_patches, embed_dim)\n","    x = self.pos_drop(x)\n","\n","    for block in self.blocks:\n","      x = block(x)\n","    \n","    x = self.norm(x)\n","    cls_token_final = x[:, 0] # (n_samples, 1, embed_dim)\n","    x = self.head(cls_token_final) \n","\n","    return x"],"metadata":{"id":"Fc47NzjgjwiF","executionInfo":{"status":"ok","timestamp":1641275559019,"user_tz":-540,"elapsed":2,"user":{"displayName":"Sukmin Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12508020700810544909"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Test with random input\n","input = torch.rand(1, 3, 384, 384).cuda()\n","model = VisionTransformer().cuda()\n","res = model(input)\n","res.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jV9itttvnCWJ","executionInfo":{"status":"ok","timestamp":1641275871188,"user_tz":-540,"elapsed":1080,"user":{"displayName":"Sukmin Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12508020700810544909"}},"outputId":"2d0cc458-424e-48d7-e549-1cf988888d53"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 1000])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":[""],"metadata":{"id":"ZU8FH1hfqf08"},"execution_count":null,"outputs":[]}]}